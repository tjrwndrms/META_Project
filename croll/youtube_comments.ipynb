{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "from googleapiclient.errors import HttpError\n",
    "import emoji\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 유튜브 API 키 설정\n",
    "# 환경 변수 가져오기\n",
    "api_key = os.getenv(\"YOUTUBE_KEY\")\n",
    "\n",
    "# 유튜브 클라이언트 생성\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "def search_videos(query, max_results=10):\n",
    "    video_ids = []\n",
    "    request = youtube.search().list(\n",
    "        part='snippet',\n",
    "        q=query,\n",
    "        type='video',\n",
    "        maxResults=50  # 한 번에 최대 50개의 결과를 요청\n",
    "    )\n",
    "    \n",
    "    while len(video_ids) < max_results:\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['id']['videoId'])\n",
    "            if len(video_ids) >= max_results:\n",
    "                break\n",
    "        \n",
    "        if 'nextPageToken' in response and len(video_ids) < max_results:\n",
    "            request = youtube.search().list(\n",
    "                part='snippet',\n",
    "                q=query,\n",
    "                type='video',\n",
    "                maxResults=50,\n",
    "                pageToken=response['nextPageToken']\n",
    "            )\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return video_ids\n",
    "\n",
    "def get_video_comments(video_id):\n",
    "    comments = []\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=100\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        while response:\n",
    "            for item in response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                comments.append(comment)\n",
    "            \n",
    "            if 'nextPageToken' in response:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    maxResults=100\n",
    "                )\n",
    "                response = request.execute()\n",
    "            else:\n",
    "                break\n",
    "    except HttpError as e:\n",
    "        print(f\"An HTTP error {e.resp.status} occurred: {e.content}\")\n",
    "    \n",
    "    return comments\n",
    "\n",
    "def filter_comments(comments, keywords):\n",
    "    filtered_comments = []\n",
    "    for comment in comments:\n",
    "        if any(keyword in comment for keyword in keywords):\n",
    "            filtered_comments.append(comment)\n",
    "    return filtered_comments\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords = set(file.read().split())\n",
    "    return stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing HTML tags, special characters, and emojis.\"\"\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # HTML 태그 제거\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 특수 문자 제거\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)  # 한글 및 공백을 제외한 문자 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나의 공백으로 변경\n",
    "    text = emoji.replace_emoji(text, replace='')  # 이모티콘 제거\n",
    "    return text\n",
    "\n",
    "def main():\n",
    "    # 검색할 키워드와 동영상 개수 입력받기\n",
    "    query = input(\"검색할 키워드를 입력하세요: \")\n",
    "    max_results = int(input(\"검색할 동영상 개수를 입력하세요: \"))\n",
    "    \n",
    "    # 입력받은 키워드로 동영상 검색\n",
    "    video_ids = search_videos(query, max_results=max_results)\n",
    "\n",
    "    # 모든 동영상의 댓글 수집 및 필터링\n",
    "    all_comments = []\n",
    "    keywords = [query]  # 필터링할 키워드 리스트에 입력받은 키워드 추가\n",
    "    \n",
    "    for video_id in tqdm(video_ids, desc=\"댓글 수집 진행 중\"):\n",
    "        comments = get_video_comments(video_id)\n",
    "        filtered_comments = filter_comments(comments, keywords)\n",
    "        all_comments.extend(filtered_comments)\n",
    "\n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame(all_comments, columns=['comment'])\n",
    "\n",
    "    # 형태소 분석기를 사용해 명사 추출\n",
    "    mecab = Mecab(dicpath=r\"C:\\\\mecab\\\\mecab-ko-dic\")\n",
    "    nouns = []\n",
    "    for comment in df['comment']:\n",
    "        cleaned_comment = clean_text(comment)\n",
    "        nouns.extend(mecab.nouns(cleaned_comment))\n",
    "    \n",
    "    # 형태소 분석 결과 확인\n",
    "    print(f\"추출된 명사: {nouns[:100]}\")  # 상위 100개의 명사 출력\n",
    "\n",
    "    # 불용어 목록 파일에서 불러오기\n",
    "    stopwords_path = 'stopwords-ko.txt'\n",
    "    korean_stopwords = load_stopwords(stopwords_path)\n",
    "\n",
    "    # 불용어 제거\n",
    "    filtered_nouns = [noun for noun in nouns if noun not in korean_stopwords]\n",
    "\n",
    "    # 명사 빈도수 계산\n",
    "    noun_counts = Counter(filtered_nouns)\n",
    "    noun_df = pd.DataFrame(noun_counts.items(), columns=['word', 'count']).sort_values(by='count', ascending=False)\n",
    "\n",
    "    # 수집된 명사 데이터 저장\n",
    "    noun_df.to_csv('filtered_youtube_comments_nouns.csv', index=False)\n",
    "\n",
    "    # 모든 명사를 하나의 문자열로 결합\n",
    "    text = ' '.join(noun_df['word'])\n",
    "\n",
    "    # 한글 폰트 경로 설정 (다운로드 받은 나눔고딕 폰트 경로)\n",
    "    font_path = 'NanumGothic.ttf'\n",
    "\n",
    "    # 제외할 단어 설정\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update(keywords)  # 필터링한 키워드를 제외 단어로 추가\n",
    "    stopwords.update(korean_stopwords)  # 불용어 목록에 추가\n",
    "\n",
    "    # 워드 클라우드 생성\n",
    "    wordcloud = WordCloud(font_path=font_path, width=800, height=400, background_color='white', stopwords=stopwords).generate(text)\n",
    "\n",
    "    # 워드 클라우드 시각화\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jeonnam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
