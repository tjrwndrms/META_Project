{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 3.48.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\gumi_env310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\Admin\\miniconda3\\envs\\gumi_env310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# STT 파일에서 텍스트 로드\n",
    "with open('./stt_file.txt', 'r', encoding='utf-8') as file:\n",
    "    stt_text = file.read()\n",
    "\n",
    "# 텍스트를 작은 조각으로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "text_chunks = text_splitter.split_text(stt_text)\n",
    "\n",
    "# 메모리 설정\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Gradio에서의 대화 함수\n",
    "def chat_with_product_info(user_input, chat_history=[]):\n",
    "    # 템플릿 정의\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"제품 정보: {input}\\n\\n질문에 대한 간단한 요약 답변(3줄 이내):\"\n",
    "    )\n",
    "\n",
    "    # LLMChain 생성\n",
    "    chat_chain = LLMChain(\n",
    "        llm=ChatOpenAI(model_name=\"gpt-4o-mini\"),\n",
    "        prompt=prompt_template,\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    # 각 텍스트 조각에 대해 질문을 처리하고 답변을 생성\n",
    "    response_summary = \"\"\n",
    "    for chunk in text_chunks:\n",
    "        combined_input = f\"{chunk}\\n질문: {user_input}\"\n",
    "        response = chat_chain.predict(input=combined_input)\n",
    "        response_summary = response.strip()  # 마지막 조각의 답변을 사용하여 요약\n",
    "\n",
    "        if response_summary:  # 첫 번째로 의미 있는 요약만 사용\n",
    "            break\n",
    "    \n",
    "    # 대화 기록에 추가\n",
    "    chat_history.append((\"사용자\", user_input))\n",
    "    chat_history.append((\"챗봇\", response_summary.strip()))\n",
    "\n",
    "    # 대화 내용을 보기 좋게 정리 (HTML 형식으로)\n",
    "    formatted_chat_history = \"\"\n",
    "    for role, message in chat_history:\n",
    "        if role == \"사용자\":\n",
    "            formatted_chat_history += f'<div style=\"text-align: right; color: gray;\"><strong>{role}</strong>: {message}</div>'\n",
    "        else:\n",
    "            formatted_chat_history += f'<div style=\"text-align: left;\"><strong>{role}</strong>: {message}</div>'\n",
    "    \n",
    "    return formatted_chat_history, chat_history, \"\"\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "def create_interface():\n",
    "    with gr.Blocks() as gr_interface:\n",
    "        with gr.Tab(\"제품\"):\n",
    "            chat_history_state = gr.State([])\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=8):\n",
    "                    chat_output = gr.HTML(label=\"채팅 기록\", interactive=False)\n",
    "                with gr.Column(scale=4):\n",
    "                    user_input = gr.Textbox(label=\"제품에 대해 질문해보세요\", placeholder=\"예: 제품의 효능은 무엇인가요?\")\n",
    "                    submit_button = gr.Button(\"제출\")\n",
    "                    submit_button.click(chat_with_product_info, inputs=[user_input, chat_history_state], outputs=[chat_output, chat_history_state, user_input])\n",
    "\n",
    "    return gr_interface\n",
    "\n",
    "# Gradio 인터페이스 실행\n",
    "if __name__ == \"__main__\":\n",
    "    gr_interface = create_interface()\n",
    "    gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gumi_env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
